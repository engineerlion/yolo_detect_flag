{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d901a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/Detection_proj/mmdetection\n"
     ]
    }
   ],
   "source": [
    "%cd mmdetection\n",
    "import mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ecc622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "dataset_type = 'CocoDataset'\n",
      "data_root = 'data/coco/'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        type='Resize',\n",
      "        img_scale=[(1333, 640), (1333, 800)],\n",
      "        multiscale_mode='value',\n",
      "        keep_ratio=True),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[102.9801, 115.9465, 122.7717],\n",
      "        std=[1.0, 1.0, 1.0],\n",
      "        to_rgb=False),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1333, 800),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[102.9801, 115.9465, 122.7717],\n",
      "                std=[1.0, 1.0, 1.0],\n",
      "                to_rgb=False),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=2,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='data/coco/annotations/instances_train2017.json',\n",
      "        img_prefix='data/coco/train2017/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                type='Resize',\n",
      "                img_scale=[(1333, 640), (1333, 800)],\n",
      "                multiscale_mode='value',\n",
      "                keep_ratio=True),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[102.9801, 115.9465, 122.7717],\n",
      "                std=[1.0, 1.0, 1.0],\n",
      "                to_rgb=False),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='data/coco/annotations/instances_val2017.json',\n",
      "        img_prefix='data/coco/val2017/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[102.9801, 115.9465, 122.7717],\n",
      "                        std=[1.0, 1.0, 1.0],\n",
      "                        to_rgb=False),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='data/coco/annotations/instances_val2017.json',\n",
      "        img_prefix='data/coco/val2017/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[102.9801, 115.9465, 122.7717],\n",
      "                        std=[1.0, 1.0, 1.0],\n",
      "                        to_rgb=False),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]))\n",
      "evaluation = dict(interval=1, metric='bbox')\n",
      "optimizer = dict(\n",
      "    type='SGD',\n",
      "    lr=0.01,\n",
      "    momentum=0.9,\n",
      "    weight_decay=0.0001,\n",
      "    paramwise_cfg=dict(bias_lr_mult=2.0, bias_decay_mult=0.0))\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='constant',\n",
      "    warmup_iters=500,\n",
      "    warmup_ratio=0.3333333333333333,\n",
      "    step=[16, 22])\n",
      "runner = dict(type='EpochBasedRunner', max_epochs=24)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "model = dict(\n",
      "    type='FCOS',\n",
      "    backbone=dict(\n",
      "        type='ResNet',\n",
      "        depth=50,\n",
      "        num_stages=4,\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        frozen_stages=1,\n",
      "        norm_cfg=dict(type='BN', requires_grad=False),\n",
      "        norm_eval=True,\n",
      "        style='caffe',\n",
      "        init_cfg=dict(\n",
      "            type='Pretrained',\n",
      "            checkpoint='open-mmlab://detectron/resnet50_caffe')),\n",
      "    neck=dict(\n",
      "        type='FPN',\n",
      "        in_channels=[256, 512, 1024, 2048],\n",
      "        out_channels=256,\n",
      "        start_level=1,\n",
      "        add_extra_convs='on_output',\n",
      "        num_outs=5,\n",
      "        relu_before_extra_convs=True),\n",
      "    bbox_head=dict(\n",
      "        type='FCOSHead',\n",
      "        num_classes=80,\n",
      "        in_channels=256,\n",
      "        stacked_convs=4,\n",
      "        feat_channels=256,\n",
      "        strides=[8, 16, 32, 64, 128],\n",
      "        loss_cls=dict(\n",
      "            type='FocalLoss',\n",
      "            use_sigmoid=True,\n",
      "            gamma=2.0,\n",
      "            alpha=0.25,\n",
      "            loss_weight=1.0),\n",
      "        loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n",
      "        loss_centerness=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)),\n",
      "    train_cfg=dict(\n",
      "        assigner=dict(\n",
      "            type='MaxIoUAssigner',\n",
      "            pos_iou_thr=0.5,\n",
      "            neg_iou_thr=0.4,\n",
      "            min_pos_iou=0,\n",
      "            ignore_iof_thr=-1),\n",
      "        allowed_border=-1,\n",
      "        pos_weight=-1,\n",
      "        debug=False),\n",
      "    test_cfg=dict(\n",
      "        nms_pre=1000,\n",
      "        min_bbox_size=0,\n",
      "        score_thr=0.05,\n",
      "        nms=dict(type='nms', iou_threshold=0.5),\n",
      "        max_per_img=100))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmdet.apis import set_random_seed\n",
    "from mmcv import Config\n",
    "from pprint import pprint\n",
    "#cfg = Config.fromfile('./configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py')\n",
    "#faster_rcnn_r50_caffe_fpn_1x_coco_bbox_mAP-0.378_20200504_180032-c5925ee5.pth\n",
    "cfg = Config.fromfile('./configs/fcos/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco.py')\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ce225e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "dataset_type = 'CocoDataset'\n",
      "data_root = '/data/flag/Images'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        type='Resize',\n",
      "        img_scale=[(1333, 640), (1333, 800)],\n",
      "        multiscale_mode='value',\n",
      "        keep_ratio=True),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[102.9801, 115.9465, 122.7717],\n",
      "        std=[1.0, 1.0, 1.0],\n",
      "        to_rgb=False),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1333, 800),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[102.9801, 115.9465, 122.7717],\n",
      "                std=[1.0, 1.0, 1.0],\n",
      "                to_rgb=False),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=2,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='/data/flag/flag_coco_train.json',\n",
      "        img_prefix='/data/flag/Images',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                type='Resize',\n",
      "                img_scale=[(1333, 640), (1333, 800)],\n",
      "                multiscale_mode='value',\n",
      "                keep_ratio=True),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[102.9801, 115.9465, 122.7717],\n",
      "                std=[1.0, 1.0, 1.0],\n",
      "                to_rgb=False),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "        ],\n",
      "        classes=[\n",
      "            'china', 'us', 'uk', 'russia', 'japan', 'france', 'german',\n",
      "            'italy', 'australia', 'korea', 'other'\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='/data/flag/flag_coco_val.json',\n",
      "        img_prefix='/data/flag/Images',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[102.9801, 115.9465, 122.7717],\n",
      "                        std=[1.0, 1.0, 1.0],\n",
      "                        to_rgb=False),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        classes=[\n",
      "            'china', 'us', 'uk', 'russia', 'japan', 'france', 'german',\n",
      "            'italy', 'australia', 'korea', 'other'\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='/data/flag/flag_coco_test.json',\n",
      "        img_prefix='/data/flag/Images',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[102.9801, 115.9465, 122.7717],\n",
      "                        std=[1.0, 1.0, 1.0],\n",
      "                        to_rgb=False),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        classes=[\n",
      "            'china', 'us', 'uk', 'russia', 'japan', 'france', 'german',\n",
      "            'italy', 'australia', 'korea', 'other'\n",
      "        ]))\n",
      "evaluation = dict(interval=1, metric=['bbox'])\n",
      "optimizer = dict(\n",
      "    type='SGD',\n",
      "    lr=0.00125,\n",
      "    momentum=0.9,\n",
      "    weight_decay=0.0001,\n",
      "    paramwise_cfg=dict(bias_lr_mult=2.0, bias_decay_mult=0.0))\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='constant',\n",
      "    warmup_iters=500,\n",
      "    warmup_ratio=0.3333333333333333,\n",
      "    step=[16, 22])\n",
      "runner = dict(type='EpochBasedRunner', max_epochs=24)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco-d92ceeea.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1), ('val', 1)]\n",
      "model = dict(\n",
      "    type='FCOS',\n",
      "    backbone=dict(\n",
      "        type='ResNet',\n",
      "        depth=50,\n",
      "        num_stages=4,\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        frozen_stages=1,\n",
      "        norm_cfg=dict(type='BN', requires_grad=False),\n",
      "        norm_eval=True,\n",
      "        style='caffe',\n",
      "        init_cfg=dict(\n",
      "            type='Pretrained',\n",
      "            checkpoint='open-mmlab://detectron/resnet50_caffe')),\n",
      "    neck=dict(\n",
      "        type='FPN',\n",
      "        in_channels=[256, 512, 1024, 2048],\n",
      "        out_channels=256,\n",
      "        start_level=1,\n",
      "        add_extra_convs='on_output',\n",
      "        num_outs=5,\n",
      "        relu_before_extra_convs=True),\n",
      "    bbox_head=dict(\n",
      "        type='FCOSHead',\n",
      "        num_classes=11,\n",
      "        in_channels=256,\n",
      "        stacked_convs=4,\n",
      "        feat_channels=256,\n",
      "        strides=[8, 16, 32, 64, 128],\n",
      "        loss_cls=dict(\n",
      "            type='FocalLoss',\n",
      "            use_sigmoid=True,\n",
      "            gamma=2.0,\n",
      "            alpha=0.25,\n",
      "            loss_weight=1.0),\n",
      "        loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n",
      "        loss_centerness=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)),\n",
      "    train_cfg=dict(\n",
      "        assigner=dict(\n",
      "            type='MaxIoUAssigner',\n",
      "            pos_iou_thr=0.5,\n",
      "            neg_iou_thr=0.4,\n",
      "            min_pos_iou=0,\n",
      "            ignore_iof_thr=-1),\n",
      "        allowed_border=-1,\n",
      "        pos_weight=-1,\n",
      "        debug=False),\n",
      "    test_cfg=dict(\n",
      "        nms_pre=1000,\n",
      "        min_bbox_size=0,\n",
      "        score_thr=0.05,\n",
      "        nms=dict(type='nms', iou_threshold=0.5),\n",
      "        max_per_img=100))\n",
      "classes = [\n",
      "    'china', 'us', 'uk', 'russia', 'japan', 'france', 'german', 'italy',\n",
      "    'australia', 'korea', 'other'\n",
      "]\n",
      "work_dir = './flag_proj/fcos'\n",
      "seed = 0\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "CLASS = ['china','us','uk','russia','japan','france','german','italy','australia','korea','other']\n",
    "                \n",
    "# Modify dataset type and path\n",
    "cfg.data_root = '/data/flag/Images'\n",
    "cfg.classes = CLASS\n",
    "\n",
    "cfg.data.test.img_prefix = '/data/flag/Images'\n",
    "cfg.data.test.ann_file = '/data/flag/flag_coco_test.json'\n",
    "cfg.data.test.classes = CLASS\n",
    "\n",
    "cfg.data.train.ann_file = '/data/flag/flag_coco_train.json'\n",
    "cfg.data.train.img_prefix = '/data/flag/Images'\n",
    "cfg.data.train.classes = CLASS\n",
    "\n",
    "cfg.data.val.img_prefix = '/data/flag/Images'\n",
    "cfg.data.val.ann_file = '/data/flag/flag_coco_val.json'\n",
    "cfg.data.val.classes = CLASS\n",
    "\n",
    "# modify num classes of the model in box head\n",
    "cfg.model.bbox_head.num_classes = 11\n",
    "#cfg.model.roi_head.mask_head.num_classes = 1\n",
    "# We can still use the pre-trained Mask RCNN model though we do not need to\n",
    "# use the mask branch\n",
    "#cfg.load_from = 'checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_\\\n",
    "#coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n",
    "cfg.load_from = 'checkpoints/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco-d92ceeea.pth'\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './flag_proj/fcos'\n",
    "\n",
    "if not os.path.exists(cfg.work_dir):\n",
    "    os.makedirs(cfg.work_dir)\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "cfg.optimizer.lr = 0.01 / 8\n",
    "#cfg.lr_config.warmup = None\n",
    "#log interval by step not epoch\n",
    "cfg.log_config.interval = 100\n",
    "\n",
    "# Change the evaluation metric since we use customized dataset.\n",
    "cfg.evaluation.metric = ['bbox']\n",
    "# We can set the evaluation interval to reduce the evaluation times\n",
    "cfg.evaluation.interval = 1\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 1\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "cfg.workflow = [('train',1),('val',1)]\n",
    "\n",
    "#cfg.runner.max_epochs = 1\n",
    "\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "print(f'Config:\\n{cfg.pretty_text}')\n",
    "cfg.dump(osp.join(cfg.work_dir, 'flag_config.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df9d23e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.12s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.apis import train_detector\n",
    "import os.path as osp\n",
    "import copy\n",
    "\n",
    "# Build dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "if len(cfg.workflow) == 2:\n",
    "    val_dataset = copy.deepcopy(cfg.data.val)\n",
    "    val_dataset.pipeline = cfg.data.train.pipeline\n",
    "    datasets.append(build_dataset(val_dataset))\n",
    "# Build the detector\n",
    "model = build_detector(\n",
    "    cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "# Add an attribute for visualization convenience\n",
    "model.CLASSES = datasets[0].CLASSES\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96457f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-06 17:52:52,259 - mmdet - INFO - load checkpoint from checkpoints/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_2x_coco-d92ceeea.pth\n",
      "2021-07-06 17:52:52,260 - mmdet - INFO - Use load_from_local loader\n",
      "2021-07-06 17:52:52,420 - mmdet - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for bbox_head.conv_cls.weight: copying a param with shape torch.Size([80, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([11, 256, 3, 3]).\n",
      "size mismatch for bbox_head.conv_cls.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([11]).\n",
      "2021-07-06 17:52:52,429 - mmdet - INFO - Start running, host: root@VM-116-54-centos, work_dir: /data/Detection_proj/mmdetection/flag_proj/fcos\n",
      "2021-07-06 17:52:52,429 - mmdet - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(NORMAL      ) NumClassCheckHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) NumClassCheckHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-07-06 17:52:52,430 - mmdet - INFO - workflow: [('train', 1), ('val', 1)], max: 24 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-06 17:54:21,304 - mmdet - INFO - Epoch [1][100/1264]\tlr: 4.167e-04, eta: 7:27:47, time: 0.889, data_time: 0.024, memory: 2946, loss_cls: 29.1947, loss_bbox: 0.4345, loss_centerness: 0.6085, loss: 30.2377, grad_norm: 222.8541\n",
      "2021-07-06 17:55:48,781 - mmdet - INFO - Epoch [1][200/1264]\tlr: 4.167e-04, eta: 7:22:50, time: 0.875, data_time: 0.003, memory: 2986, loss_cls: 0.5114, loss_bbox: 0.3287, loss_centerness: 0.5829, loss: 1.4229, grad_norm: 11.9111\n",
      "2021-07-06 17:57:16,395 - mmdet - INFO - Epoch [1][300/1264]\tlr: 4.167e-04, eta: 7:20:26, time: 0.876, data_time: 0.003, memory: 2986, loss_cls: 0.4566, loss_bbox: 0.2918, loss_centerness: 0.5767, loss: 1.3251, grad_norm: 10.3261\n"
     ]
    }
   ],
   "source": [
    "train_detector(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5d18fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:map] *",
   "language": "python",
   "name": "conda-env-map-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
